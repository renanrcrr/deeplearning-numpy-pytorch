{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af2306f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt #visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e5dc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# \"manually\" in numpy\n",
    "\n",
    "# the list of numbers\n",
    "z = [1,2,3]\n",
    "\n",
    "# compute the softmax result\n",
    "num = np.exp(z)\n",
    "den = np.sum( np.exp(z) )\n",
    "sigma = num / den\n",
    "\n",
    "print(sigma)\n",
    "print(np.sum(sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8debc07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 10  7 -1 11  2 -2  3  8 13  0  2 12 14 -5  4 -1 -5 13  2  6  7 -4 -5\n",
      "  6]\n"
     ]
    }
   ],
   "source": [
    "# repeat with some random integers\n",
    "z = np.random.randint(-5, high=15, size=25)\n",
    "print(z)\n",
    "\n",
    "# compute the softmax result\n",
    "num = np.exp(z)\n",
    "den = np.sum(num)\n",
    "sigma = num / den\n",
    "\n",
    "# compare\n",
    "plt.plot(z, sigma, 'ko')\n",
    "plt.xlabel('Original number (z)')\n",
    "plt.ylabel('Softmaxifield $\\sigma$')\n",
    "# plt.yscale('log')\n",
    "plt.title('$\\sum\\sigma$ = %g' %np.sum(sigma))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d12781ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3  1 11 10 -2  0  4 13  8  8 -4 -5  0 10 -2  9  0  3 -5  5  2 -1  6  8\n",
      "  8]\n",
      "tensor([8.7807e-08, 4.7941e-06, 1.0560e-01, 3.8847e-02, 2.3868e-07, 1.7636e-06,\n",
      "        9.6292e-05, 7.8026e-01, 5.2573e-03, 5.2573e-03, 3.2302e-08, 1.1883e-08,\n",
      "        1.7636e-06, 3.8847e-02, 2.3868e-07, 1.4291e-02, 1.7636e-06, 3.5424e-05,\n",
      "        1.1883e-08, 2.6175e-04, 1.3032e-05, 6.4881e-07, 7.1150e-04, 5.2573e-03,\n",
      "        5.2573e-03])\n"
     ]
    }
   ],
   "source": [
    "# repeat with some random integers\n",
    "z = np.random.randint(-5, high=15, size=25)\n",
    "print(z)\n",
    "\n",
    "# slightly more involved using torch.nn\n",
    "\n",
    "# create an instance of the softmax activation class\n",
    "softfun = nn.Softmax(dim=0)\n",
    "\n",
    "# then apply the data to that function\n",
    "sigmaT = softfun( torch.Tensor(z) )\n",
    "\n",
    "# now we get teh results\n",
    "print(sigmaT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ed123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the softmax result\n",
    "num = np.exp(z)\n",
    "den = np.sum( np.exp(z) )\n",
    "sigma = num / den\n",
    "\n",
    "# then apply the data to that function\n",
    "sigmaT = softfun( torch.Tensor(z) )\n",
    "\n",
    "# show that they are the same\n",
    "plt.plot(sigma, sigmaT, 'ko')\n",
    "plt.xlabel('\"Manual\", softmax')\n",
    "plt.ylabel('Pytorch nn.Softmax')\n",
    "plt.title(f'The two methods correlate at r={np.corrcoef(sigma, sigmaT)[0,1]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f8e971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
